name: Archive Aggregator Pages

on:
  workflow_dispatch:
    inputs:
      show_ids:
        description: 'Comma-separated show IDs to archive (leave empty for all shows)'
        required: false
        default: ''
      bright_data_zone:
        description: 'Bright Data zone name (optional, defaults to web_unlocker)'
        required: false
        default: 'web_unlocker'

jobs:
  archive-pages:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install axios
        run: npm install axios

      - name: Archive aggregator pages
        env:
          SCRAPINGBEE_API_KEY: ${{ secrets.SCRAPINGBEE_API_KEY }}
          BRIGHTDATA_API_KEY: ${{ secrets.BRIGHTDATA_API_KEY }}
          BRIGHTDATA_CUSTOMER_ID: ${{ secrets.BRIGHTDATA_CUSTOMER_ID }}
          BRIGHTDATA_ZONE: ${{ github.event.inputs.bright_data_zone }}
          SHOW_IDS: ${{ github.event.inputs.show_ids }}
        run: |
          node << 'EOF'
          const fs = require('fs');
          const https = require('https');
          const axios = require('axios');

          const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

          // TIER 1: ScrapingBee (primary)
          async function fetchWithScrapingBee(url, apiKey) {
            const response = await axios.get('https://app.scrapingbee.com/api/v1/', {
              params: {
                api_key: apiKey.trim(),
                url: url,
                render_js: true,
                premium_proxy: true,
                wait: 5000,
                block_ads: true,
                block_resources: false,
              },
              timeout: 30000,
            });

            return response.data;
          }

          // TIER 2: Bright Data (fallback)
          async function fetchWithBrightData(url, apiKey, customerId, zoneName) {
            // Use provided zone name or default to 'web_unlocker'
            const zone = zoneName && zoneName.trim() !== '' ? zoneName.trim() : 'web_unlocker';

            const proxy = {
              host: 'brd.superproxy.io',
              port: 33335,
              auth: {
                username: `brd-customer-${customerId.trim()}-zone-${zone}`,
                password: apiKey.trim(),
              },
            };

            const httpsAgent = new https.Agent({
              rejectUnauthorized: false,
            });

            const response = await axios.get(url, {
              proxy: proxy,
              httpsAgent: httpsAgent,
              headers: {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
              },
              timeout: 90000,
            });

            return response.data;
          }

          async function archivePage(showId, aggregator, url, config) {
            try {
              console.log(`Fetching ${aggregator} for ${showId}...`);
              console.log(`  URL: ${url}`);

              let html;
              let method;

              // Try ScrapingBee first
              if (config.scrapingBeeKey) {
                try {
                  console.log(`  Trying ScrapingBee...`);
                  html = await fetchWithScrapingBee(url, config.scrapingBeeKey);
                  method = 'scrapingbee';
                  console.log(`  ‚úì ScrapingBee succeeded`);
                } catch (sbError) {
                  console.log(`  ‚úó ScrapingBee failed: ${sbError.message}`);

                  // Fall back to Bright Data
                  if (config.brightDataKey && config.brightDataCustomerId) {
                    console.log(`  Trying Bright Data...`);
                    html = await fetchWithBrightData(url, config.brightDataKey, config.brightDataCustomerId, config.brightDataZone);
                    method = 'brightdata';
                    console.log(`  ‚úì Bright Data succeeded`);
                  } else {
                    throw sbError;
                  }
                }
              } else if (config.brightDataKey && config.brightDataCustomerId) {
                // No ScrapingBee, use Bright Data directly
                html = await fetchWithBrightData(url, config.brightDataKey, config.brightDataCustomerId, config.brightDataZone);
                method = 'brightdata';
              } else {
                throw new Error('No scraping service configured');
              }

              const dir = `data/aggregator-archive/${aggregator}`;
              if (!fs.existsSync(dir)) {
                fs.mkdirSync(dir, { recursive: true });
              }

              const timestamp = new Date().toISOString();
              const metadata = `<!-- Archived: ${timestamp} -->\n<!-- Source: ${url} -->\n<!-- Show ID: ${showId} -->\n<!-- Method: ${method} -->\n\n`;
              const content = metadata + html;

              const filepath = `${dir}/${showId}.html`;
              fs.writeFileSync(filepath, content);
              console.log(`‚úì Saved ${filepath} (via ${method})`);

              // Rate limiting: wait 2 seconds between requests
              await sleep(2000);
              return true;
            } catch (error) {
              console.error(`‚úó Failed to archive ${aggregator}/${showId}: ${error.message}`);
              return false;
            }
          }

          function generateSlug(show) {
            // Try to use existing slug, or derive from ID/title
            if (show.slug) return show.slug;

            // Remove year suffix and convert to slug format
            return show.id
              .replace(/-(bway-)?202[0-9]$/, '')
              .replace(/_/g, '-');
          }

          async function main() {
            const config = {
              scrapingBeeKey: process.env.SCRAPINGBEE_API_KEY,
              brightDataKey: process.env.BRIGHTDATA_API_KEY,
              brightDataCustomerId: process.env.BRIGHTDATA_CUSTOMER_ID,
              brightDataZone: process.env.BRIGHTDATA_ZONE || 'web_unlocker',
            };

            // Check that at least one scraping service is configured
            const hasScrapingBee = config.scrapingBeeKey && config.scrapingBeeKey.trim() !== '';
            const hasBrightData = config.brightDataKey && config.brightDataCustomerId;

            if (!hasScrapingBee && !hasBrightData) {
              console.error('‚ùå No scraping service configured. Set either:');
              console.error('   - SCRAPINGBEE_API_KEY, or');
              console.error('   - BRIGHTDATA_API_KEY + BRIGHTDATA_CUSTOMER_ID');
              process.exit(1);
            }

            console.log('üîß Configured services:');
            if (hasScrapingBee) console.log('   ‚úì ScrapingBee (primary)');
            if (hasBrightData) console.log(`   ‚úì Bright Data (fallback, zone: ${config.brightDataZone})`);

            const showsData = JSON.parse(fs.readFileSync('data/shows.json', 'utf8'));
            const shows = showsData.shows;

            let showsToProcess = shows;
            if (process.env.SHOW_IDS) {
              const selectedIds = process.env.SHOW_IDS.split(',').map(s => s.trim()).filter(Boolean);
              if (selectedIds.length > 0) {
                showsToProcess = shows.filter(show => selectedIds.includes(show.id));
                console.log(`üìã Processing ${showsToProcess.length} selected shows`);
              }
            }

            console.log(`\nüìö Archiving pages for ${showsToProcess.length} shows...\n`);

            let stats = {
              dtli: { success: 0, fail: 0 },
              showScore: { success: 0, fail: 0 },
              bww: { success: 0, fail: 0 }
            };

            for (const show of showsToProcess) {
              console.log(`\n${'='.repeat(60)}`);
              console.log(`${show.title} (${show.id})`);
              console.log('='.repeat(60));

              const slug = generateSlug(show);

              // 1. DTLI: didtheylikeit.com/shows/{slug}/
              const dtliUrl = `https://didtheylikeit.com/shows/${slug}/`;
              const dtliSuccess = await archivePage(show.id, 'dtli', dtliUrl, config);
              if (dtliSuccess) stats.dtli.success++; else stats.dtli.fail++;

              // 2. Show-Score: show-score.com/broadway-shows/{slug}
              const showScoreUrl = `https://www.show-score.com/broadway-shows/${slug}`;
              const showScoreSuccess = await archivePage(show.id, 'show-score', showScoreUrl, config);
              if (showScoreSuccess) stats.showScore.success++; else stats.showScore.fail++;

              // 3. BroadwayWorld: Search for show reviews
              const bwwSearchQuery = encodeURIComponent(`${show.title} review roundup`);
              const bwwUrl = `https://www.broadwayworld.com/search/?q=${bwwSearchQuery}`;
              const bwwSuccess = await archivePage(show.id, 'bww-roundups', bwwUrl, config);
              if (bwwSuccess) stats.bww.success++; else stats.bww.fail++;
            }

            console.log(`\n${'='.repeat(60)}`);
            console.log('üìä SUMMARY');
            console.log('='.repeat(60));
            console.log(`DTLI:       ‚úì ${stats.dtli.success}  ‚úó ${stats.dtli.fail}`);
            console.log(`Show-Score: ‚úì ${stats.showScore.success}  ‚úó ${stats.showScore.fail}`);
            console.log(`BWW:        ‚úì ${stats.bww.success}  ‚úó ${stats.bww.fail}`);
            console.log(`TOTAL:      ‚úì ${stats.dtli.success + stats.showScore.success + stats.bww.success}  ‚úó ${stats.dtli.fail + stats.showScore.fail + stats.bww.fail}`);
          }

          main().catch(error => {
            console.error('üí• Fatal error:', error);
            process.exit(1);
          });
          EOF

      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/aggregator-archive/

          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            COMMIT_MSG="Archive aggregator pages (automated)"
            if [ -n "${{ github.event.inputs.show_ids }}" ]; then
              COMMIT_MSG="Archive aggregator pages for: ${{ github.event.inputs.show_ids }}"
            fi
            git commit -m "$COMMIT_MSG"
            git push
            echo "‚úÖ Changes committed and pushed"
          fi
