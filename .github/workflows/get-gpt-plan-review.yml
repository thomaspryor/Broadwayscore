name: Get GPT-4 Plan Review

on:
  workflow_dispatch:
    inputs:
      plan_file:
        description: 'Path to plan file to review'
        required: false
        default: 'data/audit/pipeline-gap-analysis.json'

jobs:
  review:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install openai

      - name: Get GPT-4 critique
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          node << 'EOF'
          const fs = require('fs');
          const OpenAI = require('openai');

          const openai = new OpenAI();

          async function main() {
            // Read the pipeline analysis
            const pipelineAnalysis = fs.readFileSync('data/audit/pipeline-gap-analysis.json', 'utf8');

            // Read the plan file if it exists
            let planFile = '';
            try {
              planFile = fs.readFileSync('.claude/plans/snappy-leaping-rabbit.md', 'utf8');
            } catch (e) {
              console.log('No plan file found, using pipeline analysis only');
            }

            const prompt = `You are a senior data engineering consultant reviewing a Broadway review data pipeline plan. Your job is to find flaws, gaps, and potential issues.

          Here is the current pipeline analysis and sprint plans:

          ${pipelineAnalysis}

          ${planFile ? `Here is an alternative "Clean Data Collection" plan:\n\n${planFile}` : ''}

          Provide a CRITICAL review. Be harsh but constructive. Focus on:

          1. **Missing Steps**: What's NOT covered that should be?
          2. **Wrong Order**: Are steps in the wrong sequence?
          3. **Unrealistic Assumptions**: What won't work as planned?
          4. **Data Quality Risks**: What could corrupt or pollute the data?
          5. **Scalability Issues**: Will this work as the dataset grows?
          6. **Automation Gaps**: What still requires manual intervention?
          7. **Validation Weaknesses**: How could bad data slip through?
          8. **Alternative Approaches**: What would YOU do differently?

          Context that may help:
          - This is a Broadway theater review aggregator (like Rotten Tomatoes for Broadway)
          - Data sources: Show Score, Did They Like It (DTLI), BroadwayWorld (BWW)
          - DTLI and BWW thumbs are BOTH kept as inputs to LLM scoring (not competing)
          - LLM sentiment analysis is the primary scoring signal
          - Archive.org has been found to be quite successful for fetching paywalled content
          - The scraping strategy tries multiple services in order (Playwright, ScrapingBee, BrightData, Archive.org)

          Be specific. Point to exact gaps in the plans. Don't be polite - be useful.
          Format your response in Markdown.`;

            console.log('Sending to GPT-4...');

            const response = await openai.chat.completions.create({
              model: 'gpt-4o',
              messages: [{ role: 'user', content: prompt }],
              max_tokens: 4000,
              temperature: 0.7
            });

            const critique = response.choices[0].message.content;

            // Save to file for the issue
            fs.writeFileSync('/tmp/gpt-critique.md', critique);
            console.log('Critique saved to /tmp/gpt-critique.md');
            console.log('\n--- GPT-4 CRITIQUE ---\n');
            console.log(critique);
          }

          main().catch(console.error);
          EOF

      - name: Create GitHub Issue with critique
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          CRITIQUE=$(cat /tmp/gpt-critique.md)

          gh issue create \
            --title "GPT-4 Plan Review: $(date +%Y-%m-%d)" \
            --body "## GPT-4 Critique of Pipeline Plan

          This critique was generated automatically by GPT-4o reviewing our pipeline plans.

          ---

          $CRITIQUE

          ---

          *Generated by workflow run: ${{ github.run_id }}*" \
            --label "planning,ai-review"
